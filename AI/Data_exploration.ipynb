{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "provenance des donnees : https://universe.roboflow.com/roboflow-universe-projects/license-plate-recognition-rxg4e/dataset/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import draw_bounding_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../../DATA/License_Plate_Recognition/\"\n",
    "\n",
    "train_filenames = [name for name in os.listdir(path + \"train/images/\")]\n",
    "test_filenames = [name for name in os.listdir(path + \"test/images/\")]\n",
    "validation_filenames = [name for name in os.listdir(path+ \"valid/images/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filenames = train_filenames/validation_filenames/test_filenames\n",
    "#img_dir = path+\"/train/images/\" / path+\"/valid/images/\" / path+\"/test/images/\"\n",
    "#label_dir = path+\"/train/labels/\" / path+\"/valid/labels/\" / path+\"/test/labels/\"\n",
    "\n",
    "def recup_coordonnee (path):\n",
    "    f = open(path)\n",
    "    contenu = f.read()\n",
    "    f.close()\n",
    "    return contenu.split('\\n')\n",
    "\n",
    "def prep_data (coor):\n",
    "    data=[]\n",
    "    for i in range (len(coor)):\n",
    "        d=coor[i].split(' ')\n",
    "        for j in range (len(d)):\n",
    "            d[j] = (float(d[j]))\n",
    "        data.append(d)\n",
    "    return data\n",
    "\n",
    "def recup_boxe (path):\n",
    "    coor = recup_coordonnee (path)\n",
    "    coor = prep_data (coor)\n",
    "    if coor:\n",
    "        return torch.tensor(coor, dtype=torch.float32)\n",
    "    else:\n",
    "        # Retourne un tensor 0x4 (0 boîtes, 4 coordonnées)\n",
    "        return torch.empty(0, 4, dtype=torch.float32)\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, filenames, img_dir, label_dir, transform=None):\n",
    "        self.img_labels = filenames\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        if transform is None:\n",
    "            self.transform = self.default_transform\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        image = read_image(img_path)\n",
    "        label_path = os.path.join(self.label_dir, self.img_labels[idx][:-3] + 'txt')\n",
    "        label = recup_boxe(label_path)\n",
    "        image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "    def default_transform(self, image_tensor):\n",
    "        return image_tensor.float() / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomImageDataset(train_filenames, path + \"train/images/\", path + \"train/labels/\", transform=None)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = CustomImageDataset(test_filenames, path + \"test/images/\", path + \"test/labels/\", transform=None)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "validation_dataset = CustomImageDataset(validation_filenames, path + \"valid/images/\", path + \"valid/labels/\", transform=None)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 3, 640, 640])\n",
      "Labels batch shape: torch.Size([32, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[2].squeeze()\n",
    "label = train_labels[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.6587, 0.6830, 0.2313, 0.2761]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "yolo_xywhn_to_xyxy() missing 3 required positional arguments: 'h', 'W', and 'H'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 19\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor([\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, xmin), \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, ymin),\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mmin\u001b[39m(W\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, xmax), \u001b[38;5;28mmin\u001b[39m(H\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ymax)\n\u001b[1;32m     15\u001b[0m     ], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     17\u001b[0m C, H, W \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 19\u001b[0m boxes_xyxy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([yolo_xywhn_to_xyxy(\u001b[38;5;241m*\u001b[39mb, W, H) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m label], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [N,4]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# draw_bounding_boxes attend uint8 et coords en pixels\u001b[39;00m\n\u001b[1;32m     22\u001b[0m img_with_boxes \u001b[38;5;241m=\u001b[39m draw_bounding_boxes(\n\u001b[1;32m     23\u001b[0m     image\u001b[38;5;241m=\u001b[39m(img\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m255.0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39muint8),\n\u001b[1;32m     24\u001b[0m     boxes\u001b[38;5;241m=\u001b[39mboxes_xyxy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     28\u001b[0m )\n",
      "Cell \u001b[0;32mIn[84], line 19\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor([\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, xmin), \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, ymin),\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mmin\u001b[39m(W\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, xmax), \u001b[38;5;28mmin\u001b[39m(H\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ymax)\n\u001b[1;32m     15\u001b[0m     ], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     17\u001b[0m C, H, W \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 19\u001b[0m boxes_xyxy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43myolo_xywhn_to_xyxy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m label], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [N,4]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# draw_bounding_boxes attend uint8 et coords en pixels\u001b[39;00m\n\u001b[1;32m     22\u001b[0m img_with_boxes \u001b[38;5;241m=\u001b[39m draw_bounding_boxes(\n\u001b[1;32m     23\u001b[0m     image\u001b[38;5;241m=\u001b[39m(img\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m255.0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39muint8),\n\u001b[1;32m     24\u001b[0m     boxes\u001b[38;5;241m=\u001b[39mboxes_xyxy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     28\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: yolo_xywhn_to_xyxy() missing 3 required positional arguments: 'h', 'W', and 'H'"
     ]
    }
   ],
   "source": [
    "def yolo_xywhn_to_xyxy(cx, cy, w, h, W, H):\n",
    "    # centre/largeur/hauteur normalisés -> coins en pixels\n",
    "    x = cx * W\n",
    "    y = cy * H\n",
    "    bw = w * W\n",
    "    bh = h * H\n",
    "    xmin = x - bw/2\n",
    "    ymin = y - bh/2\n",
    "    xmax = x + bw/2\n",
    "    ymax = y + bh/2\n",
    "    # clamp dans l'image\n",
    "    return torch.tensor([\n",
    "        max(0, xmin), max(0, ymin),\n",
    "        min(W-1, xmax), min(H-1, ymax)\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "C, H, W = img.shape\n",
    "\n",
    "boxes_xyxy = torch.stack([yolo_xywhn_to_xyxy(*b, W, H) for b in label], dim=0)  # [N,4]\n",
    "\n",
    "# draw_bounding_boxes attend uint8 et coords en pixels\n",
    "img_with_boxes = draw_bounding_boxes(\n",
    "    image=(img*255.0).to(torch.uint8),\n",
    "    boxes=boxes_xyxy,\n",
    "    labels=[f\"obj{i}\" for i in range(len(label))],  # optionnel\n",
    "    colors=\"red\",  # optionnel\n",
    "    width=2\n",
    ")\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "plt.imshow(img_with_boxes.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recup_coordonnee1 (path):\n",
    "    f = open(path)\n",
    "    contenu = f.read()\n",
    "    f.close()\n",
    "    return contenu.split('\\n')\n",
    "\n",
    "def prep_data (coor):\n",
    "    data=[]\n",
    "    d=[]\n",
    "    for i in range (len(coor)):\n",
    "        d.append(coor[i].split(' '))\n",
    "    return d\n",
    "\n",
    "def recup_boxe (path, filenames):\n",
    "    data = []\n",
    "    for i in range (len(filenames)):\n",
    "        coor = recup_coordonnee (path+filenames[i][:-3] + 'txt')\n",
    "        coor = prep_data (coor)\n",
    "        if ('1' in coor[0] or '1' in coor[1] or '1' in coor[2] or '1' in coor[3]): \n",
    "            return True, 1\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0.6617605860916321,\n",
       "  0.67880431527482,\n",
       "  0.08537497334499947,\n",
       "  0.08642837939029935],\n",
       " [0,\n",
       "  0.3569276354232686,\n",
       "  0.6645316952542946,\n",
       "  0.07587270401018138,\n",
       "  0.07543633723582169]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recup_coordonnee (path):\n",
    "    f = open(path)\n",
    "    contenu = f.read()\n",
    "    f.close()\n",
    "    return contenu.split('\\n')\n",
    "\n",
    "def prep_data (coor):\n",
    "    data=[]\n",
    "    print (len(coor))\n",
    "    for i in range ( len(coor)):\n",
    "        d=coor[i].split(' ')\n",
    "        for j in range (len(d)):\n",
    "            d[j] = (float(d[j]))\n",
    "        d[0]=int(d[0])\n",
    "        data.append(d)\n",
    "    return data\n",
    "\n",
    "prep_data(recup_coordonnee(path + \"train/labels/\" + train_filenames[0][:-3] + 'txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0 0.6617605860916321 0.67880431527482 0.08537497334499947 0.08642837939029935\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0 0.3569276354232686 0.6645316952542946 0.07587270401018138 0.07543633723582169\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "x= ['0 0.6617605860916321 0.67880431527482 0.08537497334499947 0.08642837939029935',\n",
    " '0 0.3569276354232686 0.6645316952542946 0.07587270401018138 0.07543633723582169']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  '0.4935757205349459',\n",
       "  '0.43569868900777864',\n",
       "  '0.23294989714788858',\n",
       "  '0.2935328731437835']]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_data (recup_coordonnee1 (path + \"train/labels/\" + train_filenames[1][:-3] + 'txt'))\n",
    "#recup_boxe (path + \"train/labels/\", train_filenames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car_ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
